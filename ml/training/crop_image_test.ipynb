{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "503c583e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved leaf_frames\\frame_01.png\n",
      "Saved leaf_frames\\frame_02.png\n",
      "Saved leaf_frames\\frame_03.png\n",
      "Saved leaf_frames\\frame_04.png\n",
      "Saved leaf_frames\\frame_05.png\n",
      "Saved leaf_frames\\frame_06.png\n",
      "Generated 6 frames in 'leaf_frames' with 0% overlap, resized to max width 400px.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def crop_and_resize_leaf(image_path, output_folder, num_parts=10, overlap=0.2, max_width=600, visualize=False):\n",
    "    \"\"\"\n",
    "    Crop a leaf image into overlapping parts and resize frames for optimized stitching.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path: path to the full leaf image\n",
    "    - output_folder: folder to save cropped frames\n",
    "    - num_parts: total number of frames to generate\n",
    "    - overlap: fraction of overlap between consecutive frames (0.2–0.3 recommended)\n",
    "    - max_width: maximum width to resize frames (keeps aspect ratio)\n",
    "    - visualize: if True, display each cropped frame for debugging\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # Compute height of each cropped frame considering overlap\n",
    "    frame_height = int(h / (num_parts - (num_parts-1)*overlap))\n",
    "\n",
    "    for i in range(num_parts):\n",
    "        # Compute start and end y-coordinate with overlap\n",
    "        y_start = int(i * frame_height * (1 - overlap))\n",
    "        y_end = y_start + frame_height\n",
    "        if y_end > h:\n",
    "            y_end = h\n",
    "            y_start = max(0, h - frame_height)\n",
    "\n",
    "        cropped = img[y_start:y_end, :].copy()\n",
    "\n",
    "        # Resize for speed while keeping aspect ratio\n",
    "        current_h, current_w = cropped.shape[:2]\n",
    "        if current_w > max_width:\n",
    "            new_h = int(current_h * max_width / current_w)\n",
    "            cropped = cv2.resize(cropped, (max_width, new_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Save frame\n",
    "        filename = os.path.join(output_folder, f\"frame_{i+1:02d}.png\")\n",
    "        cv2.imwrite(filename, cropped)\n",
    "        print(f\"Saved {filename}\")\n",
    "\n",
    "        # Optional visualization for debugging\n",
    "        if visualize:\n",
    "            cv2.imshow(\"Cropped Frame\", cropped)\n",
    "            cv2.waitKey(200)\n",
    "\n",
    "    if visualize:\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    print(f\"Generated {num_parts} frames in '{output_folder}' with {int(overlap*100)}% overlap, resized to max width {max_width}px.\")\n",
    "\n",
    "# --- USAGE ---\n",
    "image_path = \"20211231_123305 (Custom).jpg\"  # your full leaf image\n",
    "output_folder = \"leaf_frames\"\n",
    "crop_and_resize_leaf(\n",
    "    image_path, \n",
    "    output_folder, \n",
    "    num_parts=6, \n",
    "    overlap=0,   # 25% overlap for more robust stitching\n",
    "    max_width=400, \n",
    "    visualize=True  # set False if you don't want preview\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1a4b489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1: ORB failed, using fallback stacking\n",
      "Frame 2: 2 keypoints, 1 matches\n",
      "Frame 2: Not enough matches, using fallback stacking\n",
      "Frame 3: 3 keypoints, 2 matches\n",
      "Frame 3: Not enough matches, using fallback stacking\n",
      "Frame 4: ORB failed, using fallback stacking\n",
      "Frame 5: ORB failed, using fallback stacking\n",
      "✅ Stitched leaf saved as stitched_leaf_test.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# -------------------------------\n",
    "# ORB + Matcher Setup\n",
    "# -------------------------------\n",
    "orb = cv2.ORB_create(1000)  # reduced features for speed\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "MAX_FRAME_WIDTH = 600  # downsize for speed\n",
    "\n",
    "def preprocess(frame, width=MAX_FRAME_WIDTH):\n",
    "    \"\"\"Resize frame to fixed width to speed up ORB\"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "    if w > width:\n",
    "        new_h = int(h * width / w)\n",
    "        frame = cv2.resize(frame, (width, new_h), interpolation=cv2.INTER_AREA)\n",
    "    return frame\n",
    "\n",
    "def linear_blend(target, warped, mask_warped):\n",
    "    \"\"\"Simple linear blending in overlap regions\"\"\"\n",
    "    overlap = (mask_warped.astype(np.uint8) & (cv2.cvtColor(target, cv2.COLOR_BGR2GRAY)>0).astype(np.uint8))\n",
    "    if overlap.sum() == 0:\n",
    "        target[mask_warped==1] = warped[mask_warped==1]\n",
    "        return target\n",
    "    dist = cv2.distanceTransform((overlap==0).astype(np.uint8), cv2.DIST_L2, 5)\n",
    "    maxd = dist.max() if dist.max()>0 else 1.0\n",
    "    alpha = np.clip(dist / maxd, 0.0, 1.0)[...,None]\n",
    "    mask = mask_warped.astype(bool)\n",
    "    target[mask] = (warped[mask].astype(np.float32) * (1-alpha[mask]) + target[mask].astype(np.float32) * alpha[mask]).astype(np.uint8)\n",
    "    return target\n",
    "\n",
    "def stitch_frames(frame_sequence):\n",
    "    \"\"\"Stitch frames robustly, with fallback if ORB fails\"\"\"\n",
    "    if len(frame_sequence) == 0:\n",
    "        raise ValueError(\"No frames provided for stitching\")\n",
    "\n",
    "    first = preprocess(frame_sequence[0])\n",
    "    H, W = first.shape[:2]\n",
    "\n",
    "    # create big canvas\n",
    "    big_canvas = np.zeros((H*4, W*3, 3), dtype=np.uint8)\n",
    "    big_mask = np.zeros((H*4, W*3), dtype=np.uint8)\n",
    "    y0 = 10\n",
    "    x0 = (big_canvas.shape[1] - W)//2\n",
    "    big_canvas[y0:y0+H, x0:x0+W] = first\n",
    "    big_mask[y0:y0+H, x0:x0+W] = 255\n",
    "\n",
    "    prev_kp, prev_des = orb.detectAndCompute(cv2.cvtColor(first, cv2.COLOR_BGR2GRAY), None)\n",
    "\n",
    "    for i, raw in enumerate(frame_sequence[1:], start=1):\n",
    "        frame = preprocess(raw)\n",
    "        gframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        kp, des = orb.detectAndCompute(gframe, None)\n",
    "        if des is None or prev_des is None:\n",
    "            print(f\"Frame {i}: ORB failed, using fallback stacking\")\n",
    "            # fallback stacking\n",
    "            ys = np.where(big_mask.any(axis=1))[0]\n",
    "            bottom = ys.max() if ys.size else 0\n",
    "            yplace = min(bottom + 5, big_canvas.shape[0] - frame.shape[0])\n",
    "            xplace = (big_canvas.shape[1] - frame.shape[1]) // 2\n",
    "            big_canvas[yplace:yplace+frame.shape[0], xplace:xplace+frame.shape[1]] = frame\n",
    "            big_mask[yplace:yplace+frame.shape[0], xplace:xplace+frame.shape[1]] = 255\n",
    "            prev_kp, prev_des = kp, des\n",
    "            continue\n",
    "\n",
    "        matches = bf.match(des, prev_des)\n",
    "        matches = sorted(matches, key=lambda x: x.distance)[:100]\n",
    "        print(f\"Frame {i}: {len(kp)} keypoints, {len(matches)} matches\")\n",
    "\n",
    "        if len(matches) < 8:\n",
    "            print(f\"Frame {i}: Not enough matches, using fallback stacking\")\n",
    "            ys = np.where(big_mask.any(axis=1))[0]\n",
    "            bottom = ys.max() if ys.size else 0\n",
    "            yplace = min(bottom + 5, big_canvas.shape[0] - frame.shape[0])\n",
    "            xplace = (big_canvas.shape[1] - frame.shape[1]) // 2\n",
    "            big_canvas[yplace:yplace+frame.shape[0], xplace:xplace+frame.shape[1]] = frame\n",
    "            big_mask[yplace:yplace+frame.shape[0], xplace:xplace+frame.shape[1]] = 255\n",
    "            prev_kp, prev_des = kp, des\n",
    "            continue\n",
    "\n",
    "        # Compute homography\n",
    "        src_pts = np.float32([ kp[m.queryIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ prev_kp[m.trainIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "        Hmat, maskH = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        if Hmat is None:\n",
    "            print(f\"Frame {i}: Homography failed, using fallback stacking\")\n",
    "            ys = np.where(big_mask.any(axis=1))[0]\n",
    "            bottom = ys.max() if ys.size else 0\n",
    "            yplace = min(bottom + 5, big_canvas.shape[0] - frame.shape[0])\n",
    "            xplace = (big_canvas.shape[1] - frame.shape[1]) // 2\n",
    "            big_canvas[yplace:yplace+frame.shape[0], xplace:xplace+frame.shape[1]] = frame\n",
    "            big_mask[yplace:yplace+frame.shape[0], xplace:xplace+frame.shape[1]] = 255\n",
    "            prev_kp, prev_des = kp, des\n",
    "            continue\n",
    "\n",
    "        # Warp and blend\n",
    "        warped = cv2.warpPerspective(frame, Hmat, (big_canvas.shape[1], big_canvas.shape[0]))\n",
    "        warped_gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "        warped_mask = (warped_gray > 10).astype(np.uint8)\n",
    "        big_canvas = linear_blend(big_canvas, warped, warped_mask)\n",
    "        big_mask = np.clip(big_mask + warped_mask*255, 0, 255)\n",
    "        prev_kp, prev_des = kp, des\n",
    "\n",
    "    # Crop non-empty region\n",
    "    ys, xs = np.where(big_mask>0)\n",
    "    if ys.size == 0:\n",
    "        return big_canvas\n",
    "    miny, maxy = ys.min(), ys.max()\n",
    "    minx, maxx = xs.min(), xs.max()\n",
    "    result = big_canvas[miny:maxy+1, minx:maxx+1]\n",
    "\n",
    "    return result\n",
    "\n",
    "# 1. Load your pre-cropped/partial leaf images\n",
    "frame_files = sorted(glob.glob(\"leaf_frames/frame_*.png\"))  # your cropped frames folder\n",
    "frames = [cv2.imread(f) for f in frame_files]\n",
    "\n",
    "# 2. Stitch frames\n",
    "stitched_leaf = stitch_frames(frames)\n",
    "\n",
    "# 3. Save and display result\n",
    "cv2.imwrite(\"stitched_leaf_test.png\", stitched_leaf)\n",
    "cv2.imshow(\"Stitched Leaf\", stitched_leaf)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"✅ Stitched leaf saved as stitched_leaf_test.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "feaec751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stitched leaf saved as 'stitched_leaf_result.png'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# --- Linear blending ---\n",
    "# -------------------------------\n",
    "def linear_blend(target, warped, mask_warped):\n",
    "    overlap = (mask_warped.astype(np.uint8) & (cv2.cvtColor(target, cv2.COLOR_BGR2GRAY)>0).astype(np.uint8))\n",
    "    if overlap.sum() == 0:\n",
    "        target[mask_warped==1] = warped[mask_warped==1]\n",
    "        return target\n",
    "    dist = cv2.distanceTransform((overlap==0).astype(np.uint8), cv2.DIST_L2, 5)\n",
    "    maxd = dist.max() if dist.max()>0 else 1.0\n",
    "    alpha = np.clip(dist / maxd, 0.0, 1.0)[...,None]\n",
    "    mask = mask_warped.astype(bool)\n",
    "    target[mask] = (warped[mask].astype(np.float32) * (1-alpha[mask]) + target[mask].astype(np.float32) * alpha[mask]).astype(np.uint8)\n",
    "    return target\n",
    "\n",
    "# -------------------------------\n",
    "# --- Robust stitching ---\n",
    "# -------------------------------\n",
    "def stitch_frames(frames):\n",
    "    if len(frames) == 0:\n",
    "        raise ValueError(\"No frames provided\")\n",
    "\n",
    "    first = frames[0]\n",
    "    H, W = first.shape[:2]\n",
    "    big_canvas = np.zeros((H*4, W*3, 3), dtype=np.uint8)\n",
    "    big_mask = np.zeros((H*4, W*3), dtype=np.uint8)\n",
    "    y0 = 10\n",
    "    x0 = (big_canvas.shape[1] - W)//2\n",
    "    big_canvas[y0:y0+H, x0:x0+W] = first\n",
    "    big_mask[y0:y0+H, x0:x0+W] = 255\n",
    "\n",
    "    detector = cv2.AKAZE_create()\n",
    "    prev_kp, prev_des = detector.detectAndCompute(cv2.cvtColor(first, cv2.COLOR_BGR2GRAY), None)\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "    for frame in frames[1:]:\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        kp, des = detector.detectAndCompute(gray, None)\n",
    "        if des is None or prev_des is None:\n",
    "            # fallback stacking\n",
    "            ys = np.where(big_mask.any(axis=1))[0]\n",
    "            bottom = ys.max() if ys.size else 0\n",
    "            yplace = min(bottom + 5, big_canvas.shape[0] - frame.shape[0])\n",
    "            xplace = (big_canvas.shape[1] - frame.shape[1]) // 2\n",
    "            big_canvas[yplace:yplace+frame.shape[0], xplace:xplace+frame.shape[1]] = frame\n",
    "            big_mask[yplace:yplace+frame.shape[0], xplace:xplace+frame.shape[1]] = 255\n",
    "            prev_kp, prev_des = kp, des\n",
    "            continue\n",
    "\n",
    "        matches = bf.match(des, prev_des)\n",
    "        matches = sorted(matches, key=lambda x: x.distance)[:150]\n",
    "\n",
    "        if len(matches) < 8:\n",
    "            # fallback stacking\n",
    "            ys = np.where(big_mask.any(axis=1))[0]\n",
    "            bottom = ys.max() if ys.size else 0\n",
    "            yplace = min(bottom + 5, big_canvas.shape[0] - frame.shape[0])\n",
    "            xplace = (big_canvas.shape[1] - frame.shape[1]) // 2\n",
    "            big_canvas[yplace:yplace+frame.shape[0], xplace:xplace+frame.shape[1]] = frame\n",
    "            big_mask[yplace:yplace+frame.shape[0], xplace:xplace+frame.shape[1]] = 255\n",
    "            prev_kp, prev_des = kp, des\n",
    "            continue\n",
    "\n",
    "        src_pts = np.float32([ kp[m.queryIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ prev_kp[m.trainIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "        Hmat, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "        if Hmat is None:\n",
    "            # fallback stacking\n",
    "            ys = np.where(big_mask.any(axis=1))[0]\n",
    "            bottom = ys.max() if ys.size else 0\n",
    "            yplace = min(bottom + 5, big_canvas.shape[0] - frame.shape[0])\n",
    "            xplace = (big_canvas.shape[1] - frame.shape[1]) // 2\n",
    "            big_canvas[yplace:yplace+frame.shape[0], xplace:xplace+frame.shape[1]] = frame\n",
    "            big_mask[yplace:yplace+frame.shape[0], xplace:xplace+frame.shape[1]] = 255\n",
    "            prev_kp, prev_des = kp, des\n",
    "            continue\n",
    "\n",
    "        warped = cv2.warpPerspective(frame, Hmat, (big_canvas.shape[1], big_canvas.shape[0]))\n",
    "        warped_gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "        warped_mask = (warped_gray > 10).astype(np.uint8)\n",
    "        big_canvas = linear_blend(big_canvas, warped, warped_mask)\n",
    "        big_mask = np.clip(big_mask + warped_mask*255, 0, 255)\n",
    "        prev_kp, prev_des = kp, des\n",
    "\n",
    "    ys, xs = np.where(big_mask>0)\n",
    "    if ys.size == 0:\n",
    "        return big_canvas\n",
    "    return big_canvas[ys.min():ys.max()+1, xs.min():xs.max()+1]\n",
    "\n",
    "# -------------------------------\n",
    "# --- Main script ---\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Load all cropped frames (replace folder path)\n",
    "    frame_files = sorted(glob.glob(\"leaf_frames/frame_*.png\"))\n",
    "    frames = [cv2.imread(f) for f in frame_files]\n",
    "\n",
    "    stitched_leaf = stitch_frames(frames)\n",
    "    cv2.imwrite(\"stitched_leaf_result.png\", stitched_leaf)\n",
    "    print(\"✅ Stitched leaf saved as 'stitched_leaf_result.png'.\")\n",
    "\n",
    "    # Optional preview\n",
    "    cv2.imshow(\"Stitched Leaf\", stitched_leaf)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d00c191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Stitched leaf saved as 'stitched_leaf_fixed.png'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "def estimate_vertical_shift(imgA, imgB):\n",
    "    colA = imgA.mean(axis=1).astype(np.float32)\n",
    "    colB = imgB.mean(axis=1).astype(np.float32)\n",
    "    # pad the shorter array to match length\n",
    "    if len(colA) != len(colB):\n",
    "        min_len = min(len(colA), len(colB))\n",
    "        colA = colA[:min_len]\n",
    "        colB = colB[:min_len]\n",
    "\n",
    "    fA = np.fft.fft(colA)\n",
    "    fB = np.fft.fft(colB)\n",
    "    R = (fA * np.conj(fB)) / (np.abs(fA * np.conj(fB)) + 1e-9)\n",
    "    r = np.fft.ifft(R)\n",
    "    shift = np.argmax(np.abs(r))\n",
    "    if shift > len(colA)//2:\n",
    "        shift -= len(colA)\n",
    "    return int(shift)\n",
    "\n",
    "def fast_stack(frames):\n",
    "    canvas = frames[0].copy()\n",
    "    for i in range(1, len(frames)):\n",
    "        A = cv2.cvtColor(canvas, cv2.COLOR_BGR2GRAY)\n",
    "        B = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
    "        # resize B to match canvas width\n",
    "        if frames[i].shape[1] != canvas.shape[1]:\n",
    "            scale = canvas.shape[1] / frames[i].shape[1]\n",
    "            new_h = int(frames[i].shape[0] * scale)\n",
    "            B = cv2.resize(B, (canvas.shape[1], new_h))\n",
    "            frames[i] = cv2.resize(frames[i], (canvas.shape[1], new_h))\n",
    "        shift = estimate_vertical_shift(A, B)\n",
    "        if shift > 0:\n",
    "            crop = frames[i][shift:, :]\n",
    "            canvas = np.vstack([canvas, crop])\n",
    "        else:\n",
    "            canvas = np.vstack([canvas, frames[i]])\n",
    "    return canvas\n",
    "\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    frame_files = sorted(glob.glob(\"leaf_frames/frame_*.png\"))\n",
    "    frames = [cv2.imread(f) for f in frame_files]\n",
    "\n",
    "    stitched_leaf = fast_stack(frames)\n",
    "    cv2.imwrite(\"stitched_leaf_fixed.png\", stitched_leaf)\n",
    "    print(\"✅ Stitched leaf saved as 'stitched_leaf_fixed.png'.\")\n",
    "\n",
    "    cv2.imshow(\"Stitched Leaf\", stitched_leaf)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c543405c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured frame 0\n",
      "Captured frame 50\n",
      "Captured frame 100\n",
      "✅ Saved stitched leaf as stitched_from_video.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "def estimate_vertical_shift(imgA, imgB):\n",
    "    colA = imgA.mean(axis=1).astype(np.float32)\n",
    "    colB = imgB.mean(axis=1).astype(np.float32)\n",
    "    min_len = min(len(colA), len(colB))\n",
    "    colA = colA[:min_len]\n",
    "    colB = colB[:min_len]\n",
    "\n",
    "    fA = np.fft.fft(colA)\n",
    "    fB = np.fft.fft(colB)\n",
    "    R = (fA * np.conj(fB)) / (np.abs(fA * np.conj(fB)) + 1e-9)\n",
    "    r = np.fft.ifft(R)\n",
    "    shift = np.argmax(np.abs(r))\n",
    "    if shift > len(colA)//2:\n",
    "        shift -= len(colA)\n",
    "    return int(shift)\n",
    "\n",
    "def fast_stack(frames):\n",
    "    canvas = frames[0].copy()\n",
    "    for i in range(1, len(frames)):\n",
    "        A = cv2.cvtColor(canvas, cv2.COLOR_BGR2GRAY)\n",
    "        B = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # resize to match width\n",
    "        if frames[i].shape[1] != canvas.shape[1]:\n",
    "            scale = canvas.shape[1] / frames[i].shape[1]\n",
    "            new_h = int(frames[i].shape[0] * scale)\n",
    "            B = cv2.resize(B, (canvas.shape[1], new_h))\n",
    "            frames[i] = cv2.resize(frames[i], (canvas.shape[1], new_h))\n",
    "\n",
    "        shift = estimate_vertical_shift(A, B)\n",
    "        if shift > 0:\n",
    "            crop = frames[i][shift:, :]\n",
    "            canvas = np.vstack([canvas, crop])\n",
    "        else:\n",
    "            canvas = np.vstack([canvas, frames[i]])\n",
    "    return canvas\n",
    "\n",
    "# -------------------------------\n",
    "def simulate_scan_from_video(video_path, frame_skip=10, max_frames=10):\n",
    "    \"\"\"\n",
    "    Simulate scanning a leaf from a video.\n",
    "    - frame_skip: how many frames to skip (controls step size)\n",
    "    - max_frames: how many frames to grab\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or len(frames) >= max_frames:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_skip == 0:\n",
    "            # resize for speed\n",
    "            frame = cv2.resize(frame, (600, int(frame.shape[0]*600/frame.shape[1])))\n",
    "            frames.append(frame)\n",
    "            print(f\"Captured frame {frame_count}\")\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"test_scan2.mp4\"  # put your test video here\n",
    "    frames = simulate_scan_from_video(video_path, frame_skip=50, max_frames=10)\n",
    "\n",
    "    if len(frames) < 2:\n",
    "        print(\"⚠️ Not enough frames extracted from video.\")\n",
    "    else:\n",
    "        stitched = fast_stack(frames)\n",
    "        cv2.imwrite(\"stitched_from_video.png\", stitched)\n",
    "        print(\"✅ Saved stitched leaf as stitched_from_video.png\")\n",
    "\n",
    "        cv2.imshow(\"Stitched Leaf\", stitched)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1cdeead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captured frame 1\n",
      "Captured frame 2\n",
      "Captured frame 3\n",
      "Captured frame 4\n",
      "Captured frame 5\n",
      "Captured frame 6\n",
      "Captured frame 7\n",
      "Captured frame 8\n",
      "Captured frame 9\n",
      "Captured frame 10\n",
      "Captured frame 11\n",
      "Captured frame 12\n",
      "Captured frame 13\n",
      "Captured frame 14\n",
      "Captured frame 15\n",
      "Frame 2: Not enough matches\n",
      "Frame 3: Not enough matches\n",
      "Frame 4: Not enough matches\n",
      "Frame 5: ORB failed, skipping\n",
      "Frame 6: ORB failed, skipping\n",
      "Frame 7: Not enough matches\n",
      "Frame 8: Not enough matches\n",
      "Frame 9: Not enough matches\n",
      "Frame 10: Not enough matches\n",
      "Frame 11: Not enough matches\n",
      "Frame 12: Not enough matches\n",
      "Frame 13: ORB failed, skipping\n",
      "Frame 14: ORB failed, skipping\n",
      "Frame 15: ORB failed, skipping\n",
      "✅ Saved stitched leaf as stitched_orb_video.png\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# ORB + BFMatcher setup\n",
    "orb = cv2.ORB_create(800)  # features per frame\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "MAX_FRAME_WIDTH = 600\n",
    "\n",
    "def preprocess(frame, width=MAX_FRAME_WIDTH):\n",
    "    h, w = frame.shape[:2]\n",
    "    if w > width:\n",
    "        new_h = int(h * width / w)\n",
    "        frame = cv2.resize(frame, (width, new_h), interpolation=cv2.INTER_AREA)\n",
    "    return frame\n",
    "\n",
    "def linear_blend(target, warped, mask_warped):\n",
    "    overlap = (mask_warped.astype(np.uint8) & (cv2.cvtColor(target, cv2.COLOR_BGR2GRAY)>0).astype(np.uint8))\n",
    "    if overlap.sum() == 0:\n",
    "        target[mask_warped==1] = warped[mask_warped==1]\n",
    "        return target\n",
    "    dist = cv2.distanceTransform((overlap==0).astype(np.uint8), cv2.DIST_L2, 5)\n",
    "    maxd = dist.max() if dist.max()>0 else 1.0\n",
    "    alpha = np.clip(dist / maxd, 0.0, 1.0)[...,None]\n",
    "    mask = mask_warped.astype(bool)\n",
    "    target[mask] = (warped[mask].astype(np.float32) * (1-alpha[mask]) + target[mask].astype(np.float32) * alpha[mask]).astype(np.uint8)\n",
    "    return target\n",
    "\n",
    "def stitch_frames(frame_sequence):\n",
    "    first = preprocess(frame_sequence[0])\n",
    "    H, W = first.shape[:2]\n",
    "    big_canvas = np.zeros((H*4, W*3, 3), dtype=np.uint8)\n",
    "    big_mask = np.zeros((H*4, W*3), dtype=np.uint8)\n",
    "\n",
    "    y0 = 50\n",
    "    x0 = (big_canvas.shape[1] - W)//2\n",
    "    big_canvas[y0:y0+H, x0:x0+W] = first\n",
    "    big_mask[y0:y0+H, x0:x0+W] = 255\n",
    "\n",
    "    prev_kp, prev_des = orb.detectAndCompute(cv2.cvtColor(first, cv2.COLOR_BGR2GRAY), None)\n",
    "\n",
    "    for i, raw in enumerate(frame_sequence[1:], start=2):\n",
    "        frame = preprocess(raw)\n",
    "        gframe = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        kp, des = orb.detectAndCompute(gframe, None)\n",
    "        if des is None or prev_des is None:\n",
    "            print(f\"Frame {i}: ORB failed, skipping\")\n",
    "            continue\n",
    "\n",
    "        matches = bf.match(des, prev_des)\n",
    "        matches = sorted(matches, key=lambda x: x.distance)[:100]\n",
    "\n",
    "        if len(matches) < 8:\n",
    "            print(f\"Frame {i}: Not enough matches\")\n",
    "            continue\n",
    "\n",
    "        src_pts = np.float32([ kp[m.queryIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "        dst_pts = np.float32([ prev_kp[m.trainIdx].pt for m in matches ]).reshape(-1,1,2)\n",
    "        Hmat, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "        if Hmat is None:\n",
    "            print(f\"Frame {i}: Homography failed\")\n",
    "            continue\n",
    "\n",
    "        warped = cv2.warpPerspective(frame, Hmat, (big_canvas.shape[1], big_canvas.shape[0]))\n",
    "        warped_gray = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "        warped_mask = (warped_gray > 10).astype(np.uint8)\n",
    "\n",
    "        big_canvas = linear_blend(big_canvas, warped, warped_mask)\n",
    "        big_mask = np.clip(big_mask + warped_mask*255, 0, 255)\n",
    "\n",
    "        prev_kp, prev_des = kp, des\n",
    "        print(f\"Frame {i}: stitched\")\n",
    "\n",
    "    ys, xs = np.where(big_mask>0)\n",
    "    if ys.size == 0:\n",
    "        return big_canvas\n",
    "    miny, maxy = ys.min(), ys.max()\n",
    "    minx, maxx = xs.min(), xs.max()\n",
    "    return big_canvas[miny:maxy+1, minx:maxx+1]\n",
    "\n",
    "# -------------------------------\n",
    "def simulate_orb_stitch_from_video(video_path, frame_skip=15, max_frames=8):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or len(frames) >= max_frames:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_skip == 0:\n",
    "            frame = cv2.resize(frame, (600, int(frame.shape[0]*600/frame.shape[1])))\n",
    "            frames.append(frame)\n",
    "            print(f\"Captured frame {len(frames)}\")\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < 2:\n",
    "        print(\"⚠️ Not enough frames for stitching\")\n",
    "        return None\n",
    "\n",
    "    stitched = stitch_frames(frames)\n",
    "    cv2.imwrite(\"stitched_orb_video.png\", stitched)\n",
    "    print(\"✅ Saved stitched leaf as stitched_orb_video.png\")\n",
    "\n",
    "    cv2.imshow(\"Stitched ORB\", stitched)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    simulate_orb_stitch_from_video(\"test_scan2.mp4\", frame_skip=10, max_frames=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.10-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
